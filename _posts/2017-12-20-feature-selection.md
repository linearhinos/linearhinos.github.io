---
layout: default
title: 线性模型时代的特征提取
---

# 特征提取Feature Selection

## 为什么要特征提取?
- 终极目的: 提高最终预估模型的准确率
- 对数据降维, 显著降低训练规模; 方便对大样本(千亿样本), 高维度数据的学习
- 不相关的特征会损害模型的预估能力, 需要去除; 好的模型在理论上可以自动识别不相关特征, 并将之权重降为0, 但干扰信息会加重模型学习的负担, 损害和误导模型的表达能力
- 同时提高了模(黑)型(盒)的可解释型, 方便badcase排查, 方便模型进一步迭代

## 特征提取面对的挑战?
- 样本量大&&维度高, 导致各种特征分析策略的效率大大降低, 需消耗较大计算资源

## 常用指标
- 方差
    - $$1/N * \sigma_1^N (ctr_i - \bar{ctr})^2 $$
    - 如性别对CTR的方差 $$ 1/2 ＊ (ctr_M - \bar{ctr})^2 + (ctr_F - \bar{ctr})^2 $$
- 相关性 = 
- 信息熵

## 特征提取策(套)略(路)
- Filter,  基于常用指标, 不依赖训练阶段本身的模型, 使用一些策略筛选特征
#### 什么是好的特征
    - 与目标强相关
    - 与其他特征弱相关
- Wrapper, 使用不同的特征组合去训练模型, 根据模型评估结果确定最佳的特征集

## 在深度学习时代, 是否还需要特征提取?
在深度学习时代之前, 如同医生看病, 基于基础指标, 特征提取是一个需要大量经验积累的玄学; 大量工程师的大量精力花在特征提取, 能挖掘好的特征是工程师/团队的核心竞争力.
但在深度学习时代, 似乎简化和解放了特征提取的玄学工作, 不用再去纠结某特征是否冗余, 是否与目标强相关;
把所有能看到的信息都塞进神经网络, 让神奇的网络告诉我们最终的答案, 不用关心输入, 不用关心学习过程(网络间的传递过于复杂, 缺乏可解释性);

特征提取的精力此时都花在了调整网络结构, 新的玄学取代了旧的玄学, 没有人说的清楚对于特定场景, 为什么A网络结构优于B;
此时, 特征提取相关策略的意义在于: 让工程师真正理解特征与目标间的关系, 在一定程度上尽量增强模型训练的可解释性
